"""Query Engine â€” äº”å±¤æ„ŸçŸ¥ RAG

æµç¨‹ï¼š
1. Frame Matching â€” åå°„åŒ¹é…ï¼ˆé—œéµå­—å‘½ä¸­ï¼‰æˆ– embedding åŒ¹é…
2. Conviction Activation â€” å‘é‡æœå°‹æ‰¾æœ€ç›¸é—œä¿¡å¿µï¼ˆæœ‰ frame æ™‚å¾ frame æ¿€æ´»ï¼‰
3. Trace Retrieval â€” ç”¨ ChromaDB å‘é‡æœå°‹æ‰¾ç›¸é—œæ¨ç†è»Œè·¡
4. Identity Check â€” ç¢ºèªä¸é•åèº«ä»½æ ¸å¿ƒ
5. Response Generation â€” ç”¨è©² frame çš„èªæ°£å’Œæ¨ç†é¢¨æ ¼ç”Ÿæˆå›æ‡‰

æ•ˆèƒ½è¨­è¨ˆï¼š
- Frame/Trace/Conviction çš„ embedding é å…ˆå»ºå¥½ç´¢å¼•ï¼ˆbuild_indexï¼‰
- æŸ¥è©¢æ™‚åªç®—ä¸€æ¬¡å•é¡Œçš„ embeddingï¼Œå…¶é¤˜ç”¨ ChromaDB å‘é‡æœå°‹
- åå°„åŒ¹é…å‘½ä¸­æ™‚å®Œå…¨è·³é embedding è¨ˆç®—
- è³‡æ–™å¿«å–ï¼šåŒä¸€ owner çš„è³‡æ–™åªè¼‰å…¥ä¸€æ¬¡ï¼Œå¾ŒçºŒæŸ¥è©¢ç›´æ¥ç”¨è¨˜æ†¶é«”
- ChromaDB client å–®ä¾‹åŒ–ï¼šåŒä¸€ owner å…±ç”¨ä¸€å€‹ client
"""

from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path

import chromadb

from engine.config import get_owner_dir
from engine.conviction_detector import _load_convictions
from engine.frame_clusterer import _load_frames
from engine.identity_scanner import _load_identity
from engine.llm import call_llm
from engine.models import (
    ContextFrame,
    Conviction,
    IdentityCore,
    ReasoningTrace,
)
from engine.signal_store import SignalStore
from engine.trace_extractor import _load_traces


@dataclass
class QueryContext:
    """æŸ¥è©¢éç¨‹ä¸­ç´¯ç©çš„ä¸Šä¸‹æ–‡ã€‚"""
    question: str
    caller: str | None = None
    matched_frame: ContextFrame | None = None
    match_method: str = ""  # "reflex" or "embedding"
    activated_convictions: list[Conviction] = field(default_factory=list)
    relevant_traces: list[ReasoningTrace] = field(default_factory=list)
    identity_constraints: list[IdentityCore] = field(default_factory=list)
    raw_signals: list[str] = field(default_factory=list)  # åŸè©±ä½è­‰
    low_confidence: bool = False  # ä¿¡å¿ƒæ ¡æº–ï¼šè­‰æ“šä¸è¶³æ™‚ç‚º True
    is_temporal: bool = False  # æ™‚åºæŸ¥è©¢æ¨™è¨˜
    response: str = ""


# â”€â”€â”€ æ™‚åºåµæ¸¬ â”€â”€â”€

_TEMPORAL_KEYWORDS = [
    "è®ŠåŒ–", "æ”¹è®Š", "ä»¥å‰", "ä¹‹å‰", "æœ€è¿‘", "ä¸€ç›´", "æ¼”è®Š", "è½‰è®Š",
    "éå»", "ç¾åœ¨", "å¾å‰", "å¾Œä¾†", "è¶¨å‹¢", "é‚„æ˜¯ä¸€æ¨£", "ä¸ä¸€æ¨£äº†",
]


def _is_temporal_query(question: str) -> bool:
    return any(kw in question for kw in _TEMPORAL_KEYWORDS)


# â”€â”€â”€ å¿«å– + å–®ä¾‹ â”€â”€â”€

_cache: dict[str, dict] = {}


def _get_cached(owner_id: str, owner_dir: Path) -> dict:
    """å–å¾—æˆ–å»ºç«‹è©² owner çš„å¿«å–ï¼ˆè³‡æ–™ + ChromaDB clientï¼‰ã€‚"""
    if owner_id not in _cache:
        chroma_dir = owner_dir / "chroma"
        _cache[owner_id] = {
            "frames": _load_frames(owner_dir),
            "convictions": _load_convictions(owner_dir),
            "traces": _load_traces(owner_dir),
            "identities": _load_identity(owner_dir),
            "conviction_map": {},
            "chroma": chromadb.PersistentClient(path=str(chroma_dir)),
        }
        _cache[owner_id]["conviction_map"] = {
            c.conviction_id: c for c in _cache[owner_id]["convictions"]
        }
    return _cache[owner_id]


def invalidate_cache(owner_id: str | None = None):
    """æ¸…é™¤å¿«å–ã€‚build_index / detect / cluster å¾Œæ‡‰å‘¼å«ã€‚"""
    if owner_id:
        _cache.pop(owner_id, None)
    else:
        _cache.clear()


# â”€â”€â”€ ç´¢å¼•ç®¡ç† â”€â”€â”€


def build_index(owner_id: str, config: dict) -> dict:
    """é å…ˆå»ºç«‹ traceã€frameã€conviction çš„ ChromaDB ç´¢å¼•ã€‚

    æ‡‰åœ¨ cluster / scan-identity / detect ä¹‹å¾ŒåŸ·è¡Œä¸€æ¬¡ã€‚
    å›å‚³çµ±è¨ˆè³‡è¨Šã€‚
    """
    owner_dir = get_owner_dir(config, owner_id)
    store = SignalStore(config, owner_id)
    chroma_dir = owner_dir / "chroma"
    client = chromadb.PersistentClient(path=str(chroma_dir))

    stats = {"traces_indexed": 0, "frames_indexed": 0, "convictions_indexed": 0}

    # --- Trace ç´¢å¼• ---
    traces = _load_traces(owner_dir)
    if traces:
        col = client.get_or_create_collection(
            name=f"{owner_id}_traces",
            metadata={"hnsw:space": "cosine"},
        )
        existing = col.get()
        if existing["ids"]:
            col.delete(ids=existing["ids"])

        ids = []
        documents = []
        metadatas = []
        for t in traces:
            text = f"{t.trigger.situation} {t.conclusion.decision}"
            ids.append(t.trace_id)
            documents.append(text)
            metadatas.append({
                "style": t.reasoning_path.style,
                "stimulus_type": t.trigger.stimulus_type,
                "context": t.source.context or "",
                "date": t.source.date,
            })

        embeddings = store._get_embedder().encode(
            documents, normalize_embeddings=True, show_progress_bar=len(documents) > 50,
        ).tolist()

        col.add(ids=ids, documents=documents, embeddings=embeddings, metadatas=metadatas)
        stats["traces_indexed"] = len(ids)

    # --- Frame ç´¢å¼• ---
    frames = _load_frames(owner_dir)
    if frames:
        col = client.get_or_create_collection(
            name=f"{owner_id}_frames",
            metadata={"hnsw:space": "cosine"},
        )
        existing = col.get()
        if existing["ids"]:
            col.delete(ids=existing["ids"])

        ids = []
        documents = []
        for f in frames:
            text = f"{f.name} {f.description}"
            for tp in f.trigger_patterns:
                text += f" {tp.pattern}"
            ids.append(f.frame_id)
            documents.append(text)

        embeddings = store._get_embedder().encode(
            documents, normalize_embeddings=True,
        ).tolist()

        col.add(ids=ids, documents=documents, embeddings=embeddings)
        stats["frames_indexed"] = len(ids)

    # --- Conviction ç´¢å¼• ---
    convictions = _load_convictions(owner_dir)
    if convictions:
        col = client.get_or_create_collection(
            name=f"{owner_id}_convictions",
            metadata={"hnsw:space": "cosine"},
        )
        existing = col.get()
        if existing["ids"]:
            col.delete(ids=existing["ids"])

        ids = []
        documents = []
        metadatas = []
        for c in convictions:
            ids.append(c.conviction_id)
            documents.append(c.statement)
            metadatas.append({
                "domain": ", ".join(c.domains) if c.domains else "",
                "strength": c.strength.score,
                "level": c.strength.level,
            })

        embeddings = store._get_embedder().encode(
            documents, normalize_embeddings=True,
            show_progress_bar=len(documents) > 50,
        ).tolist()

        col.add(ids=ids, documents=documents, embeddings=embeddings, metadatas=metadatas)
        stats["convictions_indexed"] = len(ids)

    # æ¸…é™¤å¿«å–ï¼Œä¸‹æ¬¡æŸ¥è©¢æœƒé‡æ–°è¼‰å…¥
    invalidate_cache(owner_id)

    return stats


# â”€â”€â”€ Frame Matching â”€â”€â”€


def _reflex_match(question: str, frames: list[ContextFrame]) -> ContextFrame | None:
    """åå°„åŒ¹é…ï¼šé—œéµå­—ç›´æ¥å‘½ä¸­ trigger_patterns â†’ è·³é embeddingã€‚"""
    question_lower = question.lower()
    best_frame: ContextFrame | None = None
    best_hits = 0

    for frame in frames:
        hits = 0
        for tp in frame.trigger_patterns:
            if tp.keywords:
                for kw in tp.keywords:
                    if kw.lower() in question_lower:
                        hits += 1
        if hits > best_hits:
            best_hits = hits
            best_frame = frame

    return best_frame if best_hits >= 1 else None


def _embedding_match_frame(
    question: str,
    frames: list[ContextFrame],
    q_emb: list[float],
    client: chromadb.ClientAPI,
    owner_id: str,
) -> ContextFrame | None:
    """ç”¨ ChromaDB ç´¢å¼•åŒ¹é… frameã€‚å…±ç”¨ q_emb å’Œ clientã€‚"""
    if not frames:
        return None

    frame_map = {f.frame_id: f for f in frames}

    try:
        col = client.get_collection(name=f"{owner_id}_frames")
        results = col.query(query_embeddings=[q_emb], n_results=1)
        if results["ids"] and results["ids"][0]:
            best_id = results["ids"][0][0]
            distance = results["distances"][0][0] if results.get("distances") else 1.0
            if distance < 0.7 and best_id in frame_map:
                return frame_map[best_id]
    except Exception:
        pass

    return None


# â”€â”€â”€ Conviction Activationï¼ˆå‘é‡æœå°‹ï¼‰ â”€â”€â”€


def _find_relevant_convictions(
    q_emb: list[float],
    client: chromadb.ClientAPI,
    owner_id: str,
    conviction_map: dict[str, Conviction],
    limit: int = 5,
) -> list[Conviction]:
    """ç”¨ ChromaDB ç´¢å¼•æ‰¾è·Ÿå•é¡Œæœ€ç›¸é—œçš„ convictionsã€‚"""
    try:
        col = client.get_collection(name=f"{owner_id}_convictions")
        results = col.query(query_embeddings=[q_emb], n_results=limit)
        if results["ids"] and results["ids"][0]:
            found = []
            for cid in results["ids"][0]:
                if cid in conviction_map:
                    found.append(conviction_map[cid])
            if found:
                return found
    except Exception:
        pass

    # Fallback: strength æœ€é«˜çš„ï¼ˆç´¢å¼•ä¸å­˜åœ¨æ™‚ï¼‰
    sorted_convictions = sorted(conviction_map.values(), key=lambda c: -c.strength.score)
    return sorted_convictions[:limit]


# â”€â”€â”€ Trace Retrieval â”€â”€â”€


def _find_relevant_traces(
    q_emb: list[float],
    frame: ContextFrame | None,
    client: chromadb.ClientAPI,
    owner_id: str,
    trace_map: dict[str, ReasoningTrace],
    limit: int = 5,
) -> list[ReasoningTrace]:
    """ç”¨ ChromaDB ç´¢å¼•æ‰¾ç›¸é—œ tracesã€‚å…±ç”¨ q_emb å’Œ clientã€‚"""
    # å¦‚æœæœ‰ frame ä¸” historical_traces å¤ å¤šï¼Œç›´æ¥ç”¨
    if frame and frame.reasoning_patterns.historical_traces:
        frame_trace_ids = set(frame.reasoning_patterns.historical_traces)
        frame_traces = [t for tid, t in trace_map.items() if tid in frame_trace_ids]
        if len(frame_traces) >= limit:
            return frame_traces[:limit]

    try:
        col = client.get_collection(name=f"{owner_id}_traces")
        results = col.query(query_embeddings=[q_emb], n_results=limit)
        if results["ids"] and results["ids"][0]:
            found = []
            for tid in results["ids"][0]:
                if tid in trace_map:
                    found.append(trace_map[tid])
            if found:
                return found
    except Exception:
        pass

    # Fallback: frame çš„ historical_traces
    if frame and frame.reasoning_patterns.historical_traces:
        frame_trace_ids = set(frame.reasoning_patterns.historical_traces)
        return [t for tid, t in trace_map.items() if tid in frame_trace_ids][:limit]

    # æœ€å¾Œ fallback: æŒ‰æ—¥æœŸå–æœ€è¿‘çš„
    sorted_traces = sorted(trace_map.values(), key=lambda t: t.source.date, reverse=True)
    return sorted_traces[:limit]


def _find_temporal_traces(
    q_emb: list[float],
    client: chromadb.ClientAPI,
    owner_id: str,
    trace_map: dict[str, ReasoningTrace],
    limit: int = 6,
) -> list[ReasoningTrace]:
    """æ™‚åºæŸ¥è©¢ï¼šå–ç›¸é—œ traces å¾ŒæŒ‰æ™‚é–“åˆ†æ•£ï¼Œè®“ LLM çœ‹åˆ°è®ŠåŒ–è»Œè·¡ã€‚"""
    # å…ˆç”¨å‘é‡æœå°‹æ‹¿è¼ƒå¤šå€™é¸
    candidates = []
    try:
        col = client.get_collection(name=f"{owner_id}_traces")
        results = col.query(query_embeddings=[q_emb], n_results=min(limit * 3, len(trace_map)))
        if results["ids"] and results["ids"][0]:
            for tid in results["ids"][0]:
                if tid in trace_map:
                    candidates.append(trace_map[tid])
    except Exception:
        candidates = list(trace_map.values())

    if not candidates:
        return []

    # æŒ‰æ—¥æœŸæ’åºï¼Œå–æœ€æ—©ã€ä¸­é–“ã€æœ€è¿‘å„ 1/3
    candidates.sort(key=lambda t: t.source.date)
    n = len(candidates)
    if n <= limit:
        return candidates

    third = max(1, limit // 3)
    early = candidates[:third]
    recent = candidates[-third:]
    mid_start = n // 3
    mid_end = 2 * n // 3
    middle = candidates[mid_start:mid_end][:limit - 2 * third]
    return early + middle + recent


# â”€â”€â”€ Signal å›æº¯ â”€â”€â”€


def _collect_raw_signals(
    convictions: list[Conviction],
    store: SignalStore,
    max_signals: int = 6,
) -> list[str]:
    """å¾è¢«æ¿€æ´»çš„ convictions å›æº¯åŸå§‹ signal æ–‡æœ¬ã€‚ç”¨ ChromaDB get by IDï¼Œä¸åš vector searchã€‚"""
    signal_ids: list[str] = []
    for c in convictions:
        ev = c.resonance_evidence
        for source_list in [
            ev.temporal_persistence or [],
            ev.cross_context_consistency or [],
            ev.input_output_convergence or [],
            ev.spontaneous_mentions or [],
            ev.action_alignment or [],
        ]:
            for item in source_list:
                if hasattr(item, "signal_ids"):
                    signal_ids.extend(item.signal_ids[:2])
        if len(signal_ids) >= max_signals * 2:
            break

    if not signal_ids:
        return []

    # å»é‡ï¼Œä¿æŒé †åº
    seen = set()
    unique_ids = []
    for sid in signal_ids:
        if sid not in seen:
            seen.add(sid)
            unique_ids.append(sid)
    unique_ids = unique_ids[:max_signals]

    # ChromaDB get by ID â€” O(1)ï¼Œä¸æ˜¯ vector search
    try:
        col = store._collection
        results = col.get(ids=unique_ids)
        return results.get("documents", []) or []
    except Exception:
        return []


# â”€â”€â”€ ä¿¡å¿ƒæ ¡æº– â”€â”€â”€


def _check_low_confidence(
    q_emb: list[float],
    client: chromadb.ClientAPI,
    owner_id: str,
    distance_threshold: float = 0.8,
) -> bool:
    """æª¢æŸ¥æœ€ç›¸é—œçš„ conviction å’Œ trace æ˜¯å¦éƒ½é›¢å•é¡Œå¤ªé ã€‚"""
    try:
        for col_name in [f"{owner_id}_convictions", f"{owner_id}_traces"]:
            col = client.get_collection(name=col_name)
            results = col.query(query_embeddings=[q_emb], n_results=1)
            if results["distances"] and results["distances"][0]:
                if results["distances"][0][0] < distance_threshold:
                    return False  # è‡³å°‘æœ‰ä¸€å€‹å¤ è¿‘
        return True  # å…¨éƒ¨éƒ½å¤ªé 
    except Exception:
        return False  # ç´¢å¼•ä¸å­˜åœ¨æ™‚ä¸æ¨™è¨˜


# â”€â”€â”€ Response Generation â”€â”€â”€


def _build_common_context(ctx: QueryContext) -> dict[str, str]:
    """çµ„è£å…±ç”¨çš„ prompt ç´ æï¼Œquery å’Œ generate å…±ç”¨ã€‚"""
    # ä¿¡å¿µ
    conviction_lines = []
    for c in ctx.activated_convictions:
        conviction_lines.append(f"- {c.statement}ï¼ˆstrength: {c.strength.score}ï¼‰")
    convictions_text = "\n".join(conviction_lines) if conviction_lines else "ï¼ˆç„¡ç‰¹å®šä¿¡å¿µæ¿€æ´»ï¼‰"

    # æ¨ç†è»Œè·¡ç¯„ä¾‹
    trace_lines = []
    trace_limit = 5 if not ctx.is_temporal else len(ctx.relevant_traces)
    for t in ctx.relevant_traces[:trace_limit]:
        steps = " â†’ ".join(s.action for s in t.reasoning_path.steps)
        date_prefix = f"[{t.source.date}] " if ctx.is_temporal else ""
        trace_lines.append(
            f"- {date_prefix}æƒ…å¢ƒï¼š{t.trigger.situation}\n"
            f"  æ¨ç†ï¼š{steps}ï¼ˆ{t.reasoning_path.style}ï¼‰\n"
            f"  çµè«–ï¼š{t.conclusion.decision}"
        )
    traces_text = "\n".join(trace_lines) if trace_lines else "ï¼ˆç„¡ç›¸é—œæ¨ç†è»Œè·¡ï¼‰"

    # Identity è­·æ¬„
    identity_lines = [f"- {i.core_belief}" for i in ctx.identity_constraints]
    identity_text = "\n".join(identity_lines) if identity_lines else "ï¼ˆç„¡ identity ç´„æŸï¼‰"

    # Frame è³‡è¨Š
    frame_info = ""
    if ctx.matched_frame:
        f = ctx.matched_frame
        frame_info = f"æƒ…å¢ƒæ¡†æ¶ï¼š{f.name}\næè¿°ï¼š{f.description}\n"
        if f.voice and f.voice.tone:
            frame_info += f"èªæ°£ï¼š{f.voice.tone}\n"
        if f.voice and f.voice.typical_phrases:
            frame_info += f"å¸¸ç”¨å¥å¼ï¼š{', '.join(f.voice.typical_phrases)}\n"
        if f.voice and f.voice.avoids:
            frame_info += f"é¿å…ï¼š{', '.join(f.voice.avoids)}\n"
        if f.reasoning_patterns.preferred_style:
            frame_info += f"æ¨ç†é¢¨æ ¼ï¼š{f.reasoning_patterns.preferred_style}\n"

    # åŸè©±ä½è­‰ï¼ˆsignal å›æº¯ï¼‰
    raw_signals_text = ""
    if ctx.raw_signals:
        signal_lines = [f"- ã€Œ{s[:150]}ã€" for s in ctx.raw_signals if s]
        if signal_lines:
            raw_signals_text = "é€™å€‹äººèªªéçš„åŸè©±ï¼ˆä½è­‰ï¼Œå¯é©åº¦å¼•ç”¨ï¼‰ï¼š\n" + "\n".join(signal_lines)

    # ä¿¡å¿ƒæ ¡æº–
    confidence_note = ""
    if ctx.low_confidence:
        confidence_note = (
            "\nâš ï¸ æ³¨æ„ï¼šæ­¤å•é¡Œçš„ç›¸é—œè¨˜éŒ„å¾ˆå°‘ã€‚å¦‚æœä½ ä¸ç¢ºå®šé€™å€‹äººçš„ç«‹å ´ï¼Œ"
            "è«‹å¦æ‰¿ã€Œé€™æ–¹é¢æˆ‘æ²’æœ‰æ˜ç¢ºæƒ³æ³•ã€æˆ–ã€Œæˆ‘ä¸å¤ªç¢ºå®šã€ï¼Œè€ŒéçŒœæ¸¬ã€‚\n"
        )

    # æ™‚åºæç¤º
    temporal_note = ""
    if ctx.is_temporal:
        temporal_note = (
            "\nğŸ“… é€™æ˜¯ä¸€å€‹é—œæ–¼æ™‚é–“è®ŠåŒ–çš„å•é¡Œã€‚ä¸Šæ–¹æ¨ç†è»Œè·¡å·²æŒ‰æ™‚é–“æ’åˆ—ï¼Œ"
            "è«‹é—œæ³¨ä¸åŒæ™‚æœŸçš„å·®ç•°å’Œæ¼”è®Šè¶¨å‹¢ï¼Œä¸è¦åªå–æœ€è¿‘çš„è§€é»ã€‚\n"
        )

    return {
        "convictions_text": convictions_text,
        "traces_text": traces_text,
        "identity_text": identity_text,
        "frame_info": frame_info,
        "raw_signals_text": raw_signals_text,
        "confidence_note": confidence_note,
        "temporal_note": temporal_note,
        "caller_info": f"æå•è€…ï¼š{ctx.caller}" if ctx.caller else "",
    }


def _build_response_prompt(ctx: QueryContext) -> str:
    """çµ„è£æœ€çµ‚çš„å›æ‡‰ç”Ÿæˆ promptã€‚"""
    p = _build_common_context(ctx)

    return f"""ä½ ç¾åœ¨è¦æ¨¡æ“¬ä¸€å€‹äººçš„æ€ç¶­æ–¹å¼ä¾†å›ç­”å•é¡Œã€‚

{p["frame_info"]}
{p["caller_info"]}

é€™å€‹äººçš„æ ¸å¿ƒä¿¡å¿µï¼š
{p["convictions_text"]}

èº«ä»½æ ¸å¿ƒï¼ˆåº•ç·šè­·æ¬„ï¼Œåªåœ¨å›ç­”æ˜é¡¯çŸ›ç›¾æ™‚ä¿®æ­£ï¼Œä¸è¦ä¸»å‹•ç•¶ä¸»æ—¨ç™¼æ®ï¼‰ï¼š
{p["identity_text"]}

é€™å€‹äººåœ¨é¡ä¼¼æƒ…å¢ƒä¸‹çš„æ¨ç†ç¯„ä¾‹ï¼š
{p["traces_text"]}

{p["raw_signals_text"]}
{p["confidence_note"]}{p["temporal_note"]}
å•é¡Œï¼š{ctx.question}

è«‹ç”¨é€™å€‹äººçš„æ€ç¶­æ–¹å¼ã€æ¨ç†é¢¨æ ¼å’Œèªæ°£ä¾†å›ç­”ã€‚
è¦æ±‚ï¼š
- ç”¨ç¬¬ä¸€äººç¨±ã€Œæˆ‘ã€å›ç­”
- å…§å®¹æ–¹å‘ç”±ä¸Šè¿°ä¿¡å¿µå’Œæ¨ç†è»Œè·¡ä¸»å°ï¼Œä¸è¦ç¸½æ˜¯æ”¶æŸåˆ°åŒä¸€å€‹çµè«–
- èº«ä»½æ ¸å¿ƒæ˜¯åº•ç·šè­·æ¬„ï¼šåªåœ¨å›ç­”æ˜é¡¯çŸ›ç›¾æ™‚ä¿®æ­£ï¼Œä¸è¦ä¸»å‹•æŠŠå®ƒç•¶ä¸»æ—¨ç™¼æ®
- èªæ°£è¦ç¬¦åˆæƒ…å¢ƒæ¡†æ¶çš„è¨­å®š
- é•·åº¦é©ä¸­ï¼ˆ100-300 å­—ï¼‰
- å¯ä»¥å¼•ç”¨è‡ªå·±éå»çš„æ¨ç†é‚è¼¯æˆ–åŸè©±ä½œç‚ºä½è­‰"""


def _build_generation_prompt(ctx: QueryContext, output_type: str, extra_instructions: str) -> str:
    """çµ„è£ generation mode çš„ promptã€‚"""
    p = _build_common_context(ctx)

    type_guides = {
        "article": (
            "å¯«ä¸€ç¯‡å®Œæ•´æ–‡ç« ã€‚çµæ§‹è¦æœ‰å¸å¼•äººçš„é–‹é ­ï¼ˆç”¨æ•…äº‹ã€å•é¡Œæˆ–åç›´è¦ºè§€é»åˆ‡å…¥ï¼‰ã€"
            "æœ‰é‚è¼¯çš„ä¸­æ®µï¼ˆç”¨ä¿¡å¿µå’Œæ¨ç†è»Œè·¡å±•é–‹è«–è¿°ï¼Œç©¿æ’å€‹äººç¶“é©—å’Œå…·é«”æ¡ˆä¾‹ï¼‰ã€"
            "æœ‰åŠ›çš„çµå°¾ï¼ˆå›æ‰£æ ¸å¿ƒä¿¡å¿µï¼Œçµ¦è®€è€…æ˜ç¢ºè¡Œå‹•æ–¹å‘ï¼‰ã€‚"
            "é•·åº¦ï¼š800-1500 å­—ã€‚"
        ),
        "post": (
            "å¯«ä¸€å‰‡ç¤¾ç¾¤è²¼æ–‡ã€‚é–‹é ­è¦æœ‰é‰¤å­ï¼ˆä¸€å¥è©±æŠ“ä½æ³¨æ„åŠ›ï¼‰ï¼Œ"
            "ä¸­é–“ç”¨çŸ­å¥ã€åˆ†æ®µï¼Œä¿æŒç¯€å¥æ„Ÿï¼Œçµå°¾å¸¶ call to action æˆ–å¼•ç™¼è¨è«–ã€‚"
            "é•·åº¦ï¼š200-400 å­—ã€‚"
        ),
        "decision": (
            "é‡å°é€™å€‹æ±ºç­–æƒ…å¢ƒï¼Œç”¨é€™å€‹äººçš„æ¨ç†æ–¹å¼åšåˆ†æã€‚"
            "åˆ—å‡ºæ ¸å¿ƒè€ƒé‡ã€ç”¨ä¿¡å¿µå’Œæ¨ç†é¢¨æ ¼æ¬Šè¡¡é¸é …ï¼Œçµ¦å‡ºæ˜ç¢ºå»ºè­°å’Œä¸‹ä¸€æ­¥è¡Œå‹•ã€‚"
            "é•·åº¦ï¼š300-600 å­—ã€‚"
        ),
        "script": (
            "å¯«ä¸€æ®µçŸ­å½±éŸ³è…³æœ¬ã€‚é–‹é ­ 3 ç§’è¦æœ‰å¸å¼•åŠ›çš„ hookï¼Œ"
            "ä¸­é–“ç”¨å£èªåŒ–è¡¨é”ï¼Œç¯€å¥å¿«ï¼Œæ¯æ®µä¸€å€‹é‡é»ï¼Œ"
            "çµå°¾å¸¶ CTAï¼ˆæŒ‰è®šã€ç•™è¨€ã€è¿½è¹¤ï¼‰ã€‚"
            "é•·åº¦ï¼š200-400 å­—ï¼Œæ¨™è¨»åˆ†æ®µå’Œé ä¼°ç§’æ•¸ã€‚"
        ),
    }
    format_guide = type_guides.get(output_type, type_guides["article"])

    extra_block = f"\né¡å¤–è¦æ±‚ï¼š{extra_instructions}" if extra_instructions else ""

    return f"""ä½ ç¾åœ¨è¦ç”¨ä¸€å€‹äººçš„æ€ç¶­æ–¹å¼å’Œé¢¨æ ¼ä¾†ç”¢å‡ºå…§å®¹ã€‚

{p["frame_info"]}

é€™å€‹äººçš„æ ¸å¿ƒä¿¡å¿µï¼š
{p["convictions_text"]}

èº«ä»½æ ¸å¿ƒï¼ˆåº•ç·šè­·æ¬„ï¼Œåªåœ¨å›ç­”æ˜é¡¯çŸ›ç›¾æ™‚ä¿®æ­£ï¼Œä¸è¦ä¸»å‹•ç•¶ä¸»æ—¨ç™¼æ®ï¼‰ï¼š
{p["identity_text"]}

é€™å€‹äººåœ¨é¡ä¼¼æƒ…å¢ƒä¸‹çš„æ¨ç†ç¯„ä¾‹ï¼š
{p["traces_text"]}

{p["raw_signals_text"]}
{p["confidence_note"]}{p["temporal_note"]}
ä»»å‹™ï¼š{ctx.question}

è¼¸å‡ºæ ¼å¼ï¼š{format_guide}
{extra_block}

è¦æ±‚ï¼š
- ç”¨ç¬¬ä¸€äººç¨±ã€Œæˆ‘ã€æ’°å¯«
- å…§å®¹æ–¹å‘ç”±ä¸Šè¿°ä¿¡å¿µå’Œæ¨ç†è»Œè·¡ä¸»å°ï¼Œä¸è¦ç¸½æ˜¯æ”¶æŸåˆ°åŒä¸€å€‹çµè«–
- èº«ä»½æ ¸å¿ƒæ˜¯åº•ç·šè­·æ¬„ï¼Œä¸æ˜¯æ¯ç¯‡éƒ½è¦æåˆ°çš„ä¸»æ—¨
- èªæ°£è¦ç¬¦åˆæƒ…å¢ƒæ¡†æ¶çš„è¨­å®š
- è¦æœ‰é€™å€‹äººçš„å€‹äººç‰¹è‰²ï¼šç”¨è©ç¿’æ…£ã€å¸¸ç”¨å¥å¼ã€æ€è€ƒæ–¹å¼
- è«–é»è¦å…·é«”ï¼Œç”¨æ¨ç†è»Œè·¡ä¸­çš„é‚è¼¯ã€æ¡ˆä¾‹æˆ–åŸè©±ä½è­‰ï¼Œä¸è¦ç©ºæ³›"""


# â”€â”€â”€ ä¸»å…¥å£ â”€â”€â”€


def _run_five_layer_pipeline(
    owner_id: str,
    question: str,
    caller: str | None,
    cfg: dict,
    conviction_limit: int = 5,
    trace_limit: int = 5,
) -> QueryContext:
    """å…±ç”¨çš„äº”å±¤æ„ŸçŸ¥ pipelineï¼Œquery å’Œ generate éƒ½èµ°é€™è£¡ã€‚"""
    owner_dir = get_owner_dir(cfg, owner_id)
    store = SignalStore(cfg, owner_id)
    cached = _get_cached(owner_id, owner_dir)

    ctx = QueryContext(question=question, caller=caller)

    frames = cached["frames"]
    active_frames = [f for f in frames if f.lifecycle and f.lifecycle.status == "active"]
    conviction_map = cached["conviction_map"]
    trace_map = {t.trace_id: t for t in cached["traces"]}
    client = cached["chroma"]

    # Step 1: Frame Matchingï¼ˆåå°„å„ªå…ˆï¼Œå‘½ä¸­å‰‡è·³é embeddingï¼‰
    matched = _reflex_match(question, active_frames)
    if matched:
        ctx.matched_frame = matched
        ctx.match_method = "reflex"

    # åªç®—ä¸€æ¬¡ embeddingï¼Œåå°„å‘½ä¸­æ™‚ä¸ç®—
    q_emb = None
    if not ctx.matched_frame:
        q_emb = store.compute_embedding(question)
        matched = _embedding_match_frame(question, active_frames, q_emb, client, owner_id)
        if matched:
            ctx.matched_frame = matched
            ctx.match_method = "embedding"

    # Step 2: Conviction Activationï¼ˆå‘é‡æœå°‹å–ä»£ top-strengthï¼‰
    if ctx.matched_frame:
        for ca in ctx.matched_frame.conviction_profile.primary_convictions:
            conv = conviction_map.get(ca.conviction_id)
            if conv:
                ctx.activated_convictions.append(conv)
    if not ctx.activated_convictions:
        # frame æ²’æ¿€æ´»åˆ° conviction æˆ–æ²’å‘½ä¸­ frame â†’ ç”¨å‘é‡æœå°‹
        if q_emb is None:
            q_emb = store.compute_embedding(question)
        ctx.activated_convictions = _find_relevant_convictions(
            q_emb, client, owner_id, conviction_map, limit=conviction_limit,
        )

    # Step 3: Trace Retrievalï¼ˆæ™‚åºæŸ¥è©¢èµ°ä¸åŒè·¯å¾‘ï¼‰
    if q_emb is None:
        q_emb = store.compute_embedding(question)
    ctx.is_temporal = _is_temporal_query(question)
    if ctx.is_temporal:
        ctx.relevant_traces = _find_temporal_traces(
            q_emb, client, owner_id, trace_map, limit=trace_limit,
        )
    else:
        ctx.relevant_traces = _find_relevant_traces(
            q_emb, ctx.matched_frame, client, owner_id, trace_map, limit=trace_limit,
        )

    # Step 4: Identity Check
    ctx.identity_constraints = cached["identities"]

    # Step 5: Signal å›æº¯ï¼ˆå¾ conviction æ‹¿åŸè©±ä½è­‰ï¼‰
    ctx.raw_signals = _collect_raw_signals(ctx.activated_convictions, store)

    # Step 6: ä¿¡å¿ƒæ ¡æº–ï¼ˆæª¢æŸ¥åŒ¹é…å“è³ªï¼‰
    ctx.low_confidence = _check_low_confidence(q_emb, client, owner_id)

    return ctx


def query(
    owner_id: str,
    question: str,
    caller: str | None = None,
    config: dict | None = None,
) -> dict:
    """ä¸»å…¥å£ï¼šäº”å±¤æ„ŸçŸ¥æŸ¥è©¢ã€‚"""
    from engine.config import load_config
    cfg = config or load_config()

    ctx = _run_five_layer_pipeline(owner_id, question, caller, cfg,
                                    conviction_limit=5, trace_limit=5)

    # Step 5: Response Generationï¼ˆäº”å±¤ context å·²ç²¾æº–ï¼ŒSonnet è¶³å¤ ï¼‰
    prompt = _build_response_prompt(ctx)
    ctx.response = call_llm(prompt, config=cfg, tier="medium")

    return {
        "response": ctx.response,
        "matched_frame": ctx.matched_frame.name if ctx.matched_frame else None,
        "match_method": ctx.match_method,
        "activated_convictions": [c.statement for c in ctx.activated_convictions],
        "relevant_traces": len(ctx.relevant_traces),
        "identity_constraints": [i.core_belief for i in ctx.identity_constraints],
    }


def _classify_intent(text: str) -> dict:
    """ç”¨é—œéµå­—å¿«é€Ÿåˆ¤æ–·æ„åœ–ï¼šquery vs generate + output_typeã€‚"""
    t = text.lower()

    # æ˜ç¢ºç”¢å‡ºæŒ‡ä»¤
    gen_signals = {
        "script": ["è…³æœ¬", "script", "çŸ­å½±éŸ³è…³æœ¬", "å½±ç‰‡è…³æœ¬"],
        "article": ["å¯«ä¸€ç¯‡", "å¹«æˆ‘å¯«", "å¯«æ–‡ç« ", "å¯«æ–‡", "æ’°å¯«", "ç”¢å‡ºæ–‡ç« ", "å¯«ç¨¿"],
        "post": ["è²¼æ–‡", "ç™¼æ–‡", "ç¤¾ç¾¤è²¼æ–‡", "poæ–‡", "fbè²¼æ–‡", "igè²¼æ–‡", "threads"],
        "decision": ["å¹«æˆ‘æ±ºå®š", "è©²é¸å“ªå€‹", "æ€éº¼é¸", "æ±ºç­–åˆ†æ", "å¹«æˆ‘åˆ†æè¦ä¸è¦"],
    }

    for output_type, keywords in gen_signals.items():
        for kw in keywords:
            if kw in t:
                return {"mode": "generate", "output_type": output_type}

    # é è¨­ query
    return {"mode": "query", "output_type": None}


def ask(
    owner_id: str,
    text: str,
    caller: str | None = None,
    config: dict | None = None,
) -> dict:
    """çµ±ä¸€å…¥å£ â€” è‡ªå‹•åˆ¤æ–· query æˆ– generateï¼Œè·¯ç”±åˆ°å°æ‡‰æ¨¡å¼ã€‚"""
    intent = _classify_intent(text)

    if intent["mode"] == "generate":
        result = generate(owner_id, text, output_type=intent["output_type"],
                          caller=caller, config=config)
        result["mode"] = "generate"
        return result
    else:
        result = query(owner_id, text, caller=caller, config=config)
        result["mode"] = "query"
        return result


def generate(
    owner_id: str,
    task: str,
    output_type: str = "article",
    extra_instructions: str = "",
    caller: str | None = None,
    config: dict | None = None,
) -> dict:
    """Generation Mode â€” ç”¨äº”å±¤æ€ç¶­æ¨¡å‹ç”¢å‡ºå…§å®¹æˆ–åšæ±ºç­–ã€‚

    output_type: article | post | decision | script
    """
    from engine.config import load_config
    cfg = config or load_config()

    ctx = _run_five_layer_pipeline(owner_id, task, caller, cfg,
                                    conviction_limit=7, trace_limit=8)

    # Step 5: Generationï¼ˆäº”å±¤ context å·²ç²¾æº–ï¼ŒSonnet è¶³å¤ ï¼‰
    prompt = _build_generation_prompt(ctx, output_type, extra_instructions)
    ctx.response = call_llm(prompt, config=cfg, tier="medium")

    return {
        "content": ctx.response,
        "output_type": output_type,
        "matched_frame": ctx.matched_frame.name if ctx.matched_frame else None,
        "match_method": ctx.match_method,
        "activated_convictions": [c.statement for c in ctx.activated_convictions],
        "relevant_traces": len(ctx.relevant_traces),
        "identity_constraints": [i.core_belief for i in ctx.identity_constraints],
    }
